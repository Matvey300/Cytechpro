# core/asin_search.py

import os
import time
import requests
import pandas as pd
from typing import List, Dict
from pathlib import Path
import json

SERP_API_KEY = os.getenv("SERPAPI_API_KEY")
SCRAPINGDOG_API_KEY = os.getenv("SCRAPINGDOG_API_KEY")

SERPAPI_CATEGORY_URL = "https://serpapi.com/search.json"
SCRAPINGDOG_SEARCH_URL = "https://api.scrapingdog.com/amazon/search"

def fetch_amazon_categories(keyword: str) -> list[str]:
    api_key = os.getenv("SERPAPI_API_KEY")
    if not api_key:
        raise RuntimeError("Missing SERPAPI_API_KEY")

    params = {
        "engine": "amazon",
        "amazon_domain": "amazon.com",
        "q": keyword,
        "api_key": api_key
    }

    print("ðŸ“¤ ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð² SerpApi...")
    try:
        # --- ÐÐÐ§ÐÐ›Ðž Ð”Ð˜ÐÐ“ÐÐžÐ¡Ð¢Ð˜Ð§Ð•Ð¡ÐšÐžÐ“Ðž Ð‘Ð›ÐžÐšÐ ---
        
        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
        r = requests.get(SERPAPI_CATEGORY_URL, params=params, timeout=20)

        # ÐŸÐµÑ‡Ð°Ñ‚Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾Ð± Ð¾Ñ‚Ð²ÐµÑ‚Ðµ
        print(f"âœ… Ð¡Ñ‚Ð°Ñ‚ÑƒÑ-ÐºÐ¾Ð´ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {r.status_code}")
        print("--- ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ ÑÐµÑ€Ð²ÐµÑ€Ð° ---")
        print(r.text)  # ÐŸÐµÑ‡Ð°Ñ‚Ð°ÐµÐ¼ Ð’Ð•Ð¡Ð¬ Ñ‚ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð½Ðµ JSON
        print("--- ÐšÐ¾Ð½ÐµÑ† Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ ÑÐµÑ€Ð²ÐµÑ€Ð° ---")
        
        # --- ÐšÐžÐÐ•Ð¦ Ð”Ð˜ÐÐ“ÐÐžÐ¡Ð¢Ð˜Ð§Ð•Ð¡ÐšÐžÐ“Ðž Ð‘Ð›ÐžÐšÐ ---

        if r.status_code != 200:
            print(f"[WARN] SerpAPI Ð²ÐµÑ€Ð½ÑƒÐ» ÑÑ‚Ð°Ñ‚ÑƒÑ, Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¾Ñ‚ 200.")
            return []

        data = r.json()
        print("[DEBUG] SerpAPI raw response:", json.dumps(data, indent=2))
        
        # ... Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð´ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ ...

    except requests.exceptions.RequestException as e:
        # Ð­Ñ‚Ð¾Ñ‚ Ð±Ð»Ð¾Ðº Ð¾Ñ‚Ð»Ð¾Ð²Ð¸Ñ‚ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ, SSL, Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñ‹ Ð¸ Ñ‚.Ð´.
        print(f"âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ‚ÐµÐ²Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {e}")
        return []
    
    # ... Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð´ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ ...
    categories = []
    for block in data.get("category_results", []):
        if isinstance(block, dict) and "title" in block:
            categories.append(block["title"])
    return categories

def fetch_asins_in_category(category_path: str, keyword: str, marketplace: str, max_pages: int = 5) -> List[Dict]:
    if not SCRAPINGDOG_API_KEY:
        raise RuntimeError("Missing SCRAPINGDOG_API_KEY")

    results = []
    for page in range(1, max_pages + 1):
        params = {
            "api_key": SCRAPINGDOG_API_KEY,
            "type": "search",
            "amazon_domain": f"amazon.{marketplace}",
            "query": keyword,
            "page": page,
            "category": category_path
        }

        try:
            resp = requests.get(SCRAPINGDOG_SEARCH_URL, params=params, timeout=20)
            data = resp.json().get("results", [])

            for item in data:
                if item.get("type") != "search_product":
                    continue

                asin = extract_asin_from_url(item.get("url", ""))
                if asin:
                    results.append({
                        "asin": asin,
                        "title": item.get("title"),
                        "rating": item.get("stars"),
                        "review_count": item.get("total_reviews"),
                        "category_path": category_path,
                        "country": marketplace
                    })

            time.sleep(1.5)
        except Exception as e:
            print(f"[ERROR] {category_path} page {page}: {e}")
            break

    return results


def extract_asin_from_url(url: str) -> str:
    try:
        parts = url.split("/dp/")
        if len(parts) > 1:
            return parts[1].split("/")[0]
    except:
        return None


def save_asins(df: pd.DataFrame, out_dir: Path):
    out_path = out_dir / "search_results.csv"
    df = df.drop_duplicates(subset="asin")
    df["review_count"] = pd.to_numeric(df["review_count"].str.replace(",", ""), errors="coerce")
    df = df.sort_values(by="review_count", ascending=False)
    df.head(100).to_csv(out_path, index=False)
    print(f"[âœ…] Saved top {min(len(df), 100)} ASINs to {out_path}")
